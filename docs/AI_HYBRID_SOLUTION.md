# HitlAI Hybrid AI Solution - Complete Strategy

## Executive Summary

This document consolidates the AI strategy for HitlAI, providing a clear roadmap from external APIs to a hybrid solution incorporating fine-tuned models and open-source alternatives (Mixtral, LLaMA).

**Key Principle:** Companies pay for tests â†’ Generate training data â†’ Fine-tune models â†’ Reduce costs â†’ Better quality

---

## The Hybrid Architecture

### Three-Tier Model Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Test Request                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Intelligent Router (Task Classifier)            â”‚
â”‚         Analyzes: Complexity, Vision, Cost              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚            â”‚
        â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tier 1       â”‚ â”‚ Tier 2       â”‚ â”‚ Tier 3       â”‚
â”‚ Simple Tasks â”‚ â”‚ Medium Tasks â”‚ â”‚ Complex Tasksâ”‚
â”‚ (70% volume) â”‚ â”‚ (20% volume) â”‚ â”‚ (10% volume) â”‚
â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
â”‚ Fine-tuned   â”‚ â”‚ Open Source  â”‚ â”‚ External APIsâ”‚
â”‚ GPT-4o-mini  â”‚ â”‚ Mixtral/     â”‚ â”‚ GPT-4o/      â”‚
â”‚ OR           â”‚ â”‚ LLaMA        â”‚ â”‚ Claude 3.5   â”‚
â”‚ Self-hosted  â”‚ â”‚ Self-hosted  â”‚ â”‚ Cloud APIs   â”‚
â”‚ LLaMA 8B     â”‚ â”‚              â”‚ â”‚              â”‚
â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
â”‚ $0.01-0.05   â”‚ â”‚ $0.05-0.15   â”‚ â”‚ $0.20-0.50   â”‚
â”‚ per test     â”‚ â”‚ per test     â”‚ â”‚ per test     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Model Options & Configurations

### Tier 1: Simple Tasks (70% of workload)

**Use Cases:**
- Selector generation
- Data extraction
- Classification
- Simple issue detection
- Form validation checks

**Option A: Fine-Tuned GPT-4o-mini (Recommended for Phase 2)**
```typescript
{
  provider: 'openai',
  model: 'ft:gpt-4o-mini:hitlai:ux-detector:v1',
  costPer1kTokens: 0.0001,
  hosting: 'OpenAI Cloud',
  setup: 'Easy (API only)',
  maintenance: 'None',
  quality: 'High',
  latency: '200-500ms'
}
```

**Option B: Self-Hosted LLaMA 3.1 8B (Phase 3)**
```typescript
{
  provider: 'meta',
  model: 'llama-3.1-8b-instruct',
  costPer1kTokens: 0.00005,
  hosting: 'AWS/GCP GPU',
  setup: 'Complex (Infrastructure)',
  maintenance: 'High',
  quality: 'Medium-High',
  latency: '100-300ms',
  infrastructure: '$500-1000/month'
}
```

**Option C: Groq-Hosted LLaMA (Phase 2.5)**
```typescript
{
  provider: 'groq',
  model: 'llama-3.1-8b-instant',
  costPer1kTokens: 0.00005,
  hosting: 'Groq Cloud',
  setup: 'Easy (API only)',
  maintenance: 'None',
  quality: 'Medium-High',
  latency: '50-150ms (very fast)',
  note: 'Good middle ground'
}
```

---

### Tier 2: Medium Complexity (20% of workload)

**Use Cases:**
- Test strategy planning
- Behavior analysis
- Synthetic session generation
- Screenshot analysis (non-critical)
- Recommendation generation

**Option A: Mixtral 8x7B (Recommended for Phase 3)**
```typescript
{
  provider: 'mistral',
  model: 'mixtral-8x7b-instruct',
  costPer1kTokens: 0.0002,
  hosting: 'Self-hosted or Together.ai',
  setup: 'Medium',
  maintenance: 'Medium',
  quality: 'High',
  latency: '300-600ms',
  infrastructure: '$1500-3000/month',
  note: 'Excellent quality/cost ratio'
}
```

**Option B: Fine-Tuned GPT-4o (Phase 2)**
```typescript
{
  provider: 'openai',
  model: 'ft:gpt-4o:hitlai:strategy-planner:v1',
  costPer1kTokens: 0.002,
  hosting: 'OpenAI Cloud',
  setup: 'Easy',
  maintenance: 'None',
  quality: 'Very High',
  latency: '300-800ms'
}
```

**Option C: LLaMA 3.1 70B (Phase 3)**
```typescript
{
  provider: 'meta',
  model: 'llama-3.1-70b-instruct',
  costPer1kTokens: 0.0003,
  hosting: 'AWS/GCP GPU (A100)',
  setup: 'Complex',
  maintenance: 'High',
  quality: 'Very High',
  latency: '400-1000ms',
  infrastructure: '$3000-6000/month',
  note: 'Best open-source quality'
}
```

---

### Tier 3: Complex Tasks (10% of workload)

**Use Cases:**
- Vision analysis (critical)
- Critique and meta-analysis
- Divergence analysis
- Novel UX pattern detection
- High-stakes recommendations

**Option A: Claude 3.5 Sonnet (Recommended)**
```typescript
{
  provider: 'anthropic',
  model: 'claude-3-5-sonnet-20241022',
  costPer1kTokens: 0.003,
  hosting: 'Anthropic Cloud',
  setup: 'Easy',
  maintenance: 'None',
  quality: 'Excellent',
  latency: '500-1500ms',
  capabilities: ['vision', 'critique', 'complex_reasoning']
}
```

**Option B: GPT-4o (Alternative)**
```typescript
{
  provider: 'openai',
  model: 'gpt-4o',
  costPer1kTokens: 0.0025,
  hosting: 'OpenAI Cloud',
  setup: 'Easy',
  maintenance: 'None',
  quality: 'Excellent',
  latency: '400-1200ms',
  capabilities: ['vision', 'function_calling', 'complex_reasoning']
}
```

**Option C: LLaMA 3.2 Vision (Phase 3, Experimental)**
```typescript
{
  provider: 'meta',
  model: 'llama-3.2-90b-vision-instruct',
  costPer1kTokens: 0.0005,
  hosting: 'Self-hosted (Multiple A100s)',
  setup: 'Very Complex',
  maintenance: 'Very High',
  quality: 'Good (improving)',
  latency: '1000-2000ms',
  infrastructure: '$8000-15000/month',
  note: 'Not recommended yet - vision quality below GPT-4o/Claude'
}
```

---

## Phase-by-Phase Implementation

### Phase 1: External APIs Only (Months 0-3) âœ… CURRENT

**Status:** Implemented

**Models:**
- GPT-4o-mini: Simple tasks
- GPT-4o: Medium tasks
- Claude 3.5 Sonnet: Complex tasks + Vision

**Cost per test:** ~$1.05
**Infrastructure:** $0 (API only)
**Total at 10k tests/month:** $10,500

**What to do:**
- âœ… Use tiered reasoning (already implemented)
- âœ… Collect training data from every test
- âœ… Track human feedback and labels
- âœ… Build data quality pipeline

**Goal:** Reach 1,000 high-quality labeled tests

---

### Phase 2: Fine-Tuned External APIs (Months 3-9) ğŸ¯ NEXT

**Status:** Infrastructure ready, pipeline needed

**Models:**
- **Tier 1:** `ft:gpt-4o-mini:hitlai:ux-detector:v1` (fine-tuned)
- **Tier 2:** `ft:gpt-4o:hitlai:strategy-planner:v1` (fine-tuned)
- **Tier 3:** Claude 3.5 Sonnet (unchanged)

**Cost per test:** ~$0.50-0.75 (30-50% reduction)
**Infrastructure:** $0 (still API only)
**Fine-tuning cost:** $500-1000/month (4 models updated monthly)
**Total at 10k tests/month:** $5,500-8,000

**What to build:**
1. Training data collection pipeline
2. Data export in OpenAI format (JSONL)
3. Fine-tuning automation
4. A/B testing framework
5. Model versioning system

**Goal:** 50% cost reduction while improving quality

---

### Phase 2.5: Add Groq for Speed (Months 6-9) âš¡ OPTIONAL

**Status:** Easy addition

**Models:**
- **Tier 1:** Groq-hosted LLaMA 3.1 8B (for speed-critical tasks)
- **Tier 2 & 3:** Same as Phase 2

**Why Groq:**
- 10x faster inference (50-150ms vs 500ms)
- Same cost as self-hosting
- No infrastructure management
- Good for real-time features

**Cost per test:** ~$0.40-0.60
**Total at 10k tests/month:** $4,500-6,500

**Implementation:**
```typescript
// Add to tieredReasoning.ts
'llama-3.1-8b-groq': {
  provider: 'groq',
  model: 'llama-3.1-8b-instant',
  costPer1kTokens: 0.00005,
  capabilities: ['text', 'json', 'fast_inference']
}
```

---

### Phase 3: Hybrid (Self-Hosted + External) (Months 9-18) ğŸš€ FUTURE

**Status:** Not started

**Architecture:**
```
Tier 1 (70%): Self-hosted LLaMA 3.1 8B
Tier 2 (20%): Self-hosted Mixtral 8x7B
Tier 3 (10%): Claude 3.5 Sonnet (external)
```

**Cost per test:** ~$0.30-0.50 (50-70% reduction)
**Infrastructure:** $4,000-8,000/month (GPUs)
**Total at 10k tests/month:** $7,000-10,000
**Total at 100k tests/month:** $34,000-50,000 (vs $105,000 external only)

**Infrastructure Requirements:**

**Tier 1 (LLaMA 8B):**
- 2x NVIDIA A10G (24GB VRAM each)
- AWS: g5.2xlarge instances
- Cost: ~$1,500/month
- Handles: 70% of requests

**Tier 2 (Mixtral 8x7B):**
- 2x NVIDIA A100 (40GB VRAM each)
- AWS: p4d.24xlarge (shared)
- Cost: ~$2,500/month
- Handles: 20% of requests

**Load Balancing:**
- Kubernetes for orchestration
- Auto-scaling based on load
- Fallback to external APIs if overloaded

**What to build:**
1. Model serving infrastructure (vLLM or TGI)
2. Load balancing and routing
3. Monitoring and alerting
4. Auto-scaling logic
5. Fallback mechanisms

**Only do this if:**
- You have 50k+ tests/month
- API costs exceed $50k/month
- You have DevOps/ML team
- Cost savings justify infrastructure complexity

---

## Model Comparison Matrix

| Model | Provider | Cost/1k | Quality | Speed | Vision | Self-Host | Best For |
|-------|----------|---------|---------|-------|--------|-----------|----------|
| **GPT-4o-mini** | OpenAI | $0.00015 | High | Medium | No | No | Simple tasks |
| **ft:gpt-4o-mini** | OpenAI | $0.0001 | Very High* | Medium | No | No | Simple tasks (fine-tuned) |
| **LLaMA 3.1 8B** | Meta | $0.00005 | Medium-High | Fast | No | Yes | Simple tasks (self-hosted) |
| **LLaMA 8B (Groq)** | Groq | $0.00005 | Medium-High | Very Fast | No | No | Speed-critical tasks |
| **GPT-4o** | OpenAI | $0.0025 | Excellent | Medium | Yes | No | Medium-complex tasks |
| **Mixtral 8x7B** | Mistral | $0.0002 | High | Medium | No | Yes | Medium tasks (best value) |
| **LLaMA 3.1 70B** | Meta | $0.0003 | Very High | Slow | No | Yes | Complex tasks (self-hosted) |
| **Claude 3.5 Sonnet** | Anthropic | $0.003 | Excellent | Medium | Yes | No | Vision + Critique |
| **LLaMA 3.2 90B Vision** | Meta | $0.0005 | Good | Very Slow | Yes | Yes | Vision (not ready yet) |

*Quality is "Very High" for YOUR specific use case after fine-tuning

---

## Cost Analysis: All Scenarios

### Scenario 1: 10,000 tests/month

| Phase | Setup | Cost/Test | Monthly Cost | Savings |
|-------|-------|-----------|--------------|---------|
| Phase 1 (Current) | External APIs only | $1.05 | $10,500 | Baseline |
| Phase 2 | Fine-tuned APIs | $0.65 | $6,500 | $4,000 (38%) |
| Phase 2.5 | + Groq | $0.50 | $5,000 | $5,500 (52%) |
| Phase 3 | Hybrid (self-hosted) | $0.70 | $7,000* | $3,500 (33%) |

*Phase 3 costs more at low volume due to infrastructure overhead

**Recommendation:** Phase 2 or 2.5 (fine-tuned + Groq)

---

### Scenario 2: 50,000 tests/month

| Phase | Setup | Cost/Test | Monthly Cost | Savings |
|-------|-------|-----------|--------------|---------|
| Phase 1 | External APIs only | $1.05 | $52,500 | Baseline |
| Phase 2 | Fine-tuned APIs | $0.65 | $32,500 | $20,000 (38%) |
| Phase 2.5 | + Groq | $0.50 | $25,000 | $27,500 (52%) |
| Phase 3 | Hybrid (self-hosted) | $0.40 | $20,000 + $6k infra = $26,000 | $26,500 (50%) |

**Recommendation:** Phase 2.5 or Phase 3 (similar costs, Phase 3 gives more control)

---

### Scenario 3: 100,000 tests/month

| Phase | Setup | Cost/Test | Monthly Cost | Savings |
|-------|-------|-----------|--------------|---------|
| Phase 1 | External APIs only | $1.05 | $105,000 | Baseline |
| Phase 2 | Fine-tuned APIs | $0.65 | $65,000 | $40,000 (38%) |
| Phase 2.5 | + Groq | $0.50 | $50,000 | $55,000 (52%) |
| Phase 3 | Hybrid (self-hosted) | $0.34 | $34,000 + $8k infra = $42,000 | $63,000 (60%) |

**Recommendation:** Phase 3 (hybrid) - clear winner at scale

---

## Implementation Code Examples

### 1. Enhanced Tiered Reasoning with All Models

```typescript
// lib/optimization/tieredReasoning.ts

export class TieredReasoning {
  private static models: Record<string, ModelConfig> = {
    // Phase 1: External APIs
    'gpt-4o-mini': {
      provider: 'openai',
      model: 'gpt-4o-mini',
      costPer1kTokens: 0.00015,
      tier: 1,
      capabilities: ['text', 'json', 'function_calling']
    },
    'gpt-4o': {
      provider: 'openai',
      model: 'gpt-4o',
      costPer1kTokens: 0.0025,
      tier: 2,
      capabilities: ['text', 'json', 'vision', 'complex_reasoning']
    },
    'claude-3-5-sonnet': {
      provider: 'anthropic',
      model: 'claude-3-5-sonnet-20241022',
      costPer1kTokens: 0.003,
      tier: 3,
      capabilities: ['text', 'json', 'vision', 'critique']
    },
    
    // Phase 2: Fine-tuned models
    'ft-ux-detector': {
      provider: 'openai',
      model: 'ft:gpt-4o-mini:hitlai:ux-detector:v1',
      costPer1kTokens: 0.0001,
      tier: 1,
      capabilities: ['text', 'json', 'issue_detection']
    },
    'ft-strategy-planner': {
      provider: 'openai',
      model: 'ft:gpt-4o:hitlai:strategy-planner:v1',
      costPer1kTokens: 0.002,
      tier: 2,
      capabilities: ['text', 'json', 'strategy']
    },
    
    // Phase 2.5: Groq
    'llama-8b-groq': {
      provider: 'groq',
      model: 'llama-3.1-8b-instant',
      costPer1kTokens: 0.00005,
      tier: 1,
      capabilities: ['text', 'json', 'fast_inference']
    },
    
    // Phase 3: Self-hosted
    'llama-8b-self': {
      provider: 'self-hosted',
      model: 'llama-3.1-8b-instruct',
      costPer1kTokens: 0.00003,
      tier: 1,
      endpoint: process.env.LLAMA_8B_ENDPOINT,
      capabilities: ['text', 'json']
    },
    'mixtral-8x7b-self': {
      provider: 'self-hosted',
      model: 'mixtral-8x7b-instruct',
      costPer1kTokens: 0.0001,
      tier: 2,
      endpoint: process.env.MIXTRAL_ENDPOINT,
      capabilities: ['text', 'json', 'complex_reasoning']
    },
    'llama-70b-self': {
      provider: 'self-hosted',
      model: 'llama-3.1-70b-instruct',
      costPer1kTokens: 0.0002,
      tier: 2,
      endpoint: process.env.LLAMA_70B_ENDPOINT,
      capabilities: ['text', 'json', 'complex_reasoning']
    }
  }

  static selectModel(
    taskType: string,
    complexity: TaskComplexity,
    requiresVision: boolean = false,
    phase: 'phase1' | 'phase2' | 'phase2.5' | 'phase3' = 'phase1'
  ): { modelKey: string; config: ModelConfig } {
    
    // Vision always uses external APIs (for now)
    if (requiresVision) {
      return complexity === 'critical'
        ? { modelKey: 'claude-3-5-sonnet', config: this.models['claude-3-5-sonnet'] }
        : { modelKey: 'gpt-4o', config: this.models['gpt-4o'] }
    }

    // Route based on phase
    switch (phase) {
      case 'phase1':
        return this.selectPhase1Model(taskType, complexity)
      
      case 'phase2':
        return this.selectPhase2Model(taskType, complexity)
      
      case 'phase2.5':
        return this.selectPhase2_5Model(taskType, complexity)
      
      case 'phase3':
        return this.selectPhase3Model(taskType, complexity)
      
      default:
        return this.selectPhase1Model(taskType, complexity)
    }
  }

  private static selectPhase1Model(taskType: string, complexity: TaskComplexity) {
    // Current implementation (already exists)
    if (taskType === 'critique') {
      return { modelKey: 'claude-3-5-sonnet', config: this.models['claude-3-5-sonnet'] }
    }
    
    if (complexity === 'simple') {
      return { modelKey: 'gpt-4o-mini', config: this.models['gpt-4o-mini'] }
    }
    
    return { modelKey: 'gpt-4o', config: this.models['gpt-4o'] }
  }

  private static selectPhase2Model(taskType: string, complexity: TaskComplexity) {
    // Use fine-tuned models where available
    if (taskType === 'issue_detection' || taskType === 'classification') {
      return { modelKey: 'ft-ux-detector', config: this.models['ft-ux-detector'] }
    }
    
    if (taskType === 'test_strategy' || taskType === 'heuristic_weighting') {
      return { modelKey: 'ft-strategy-planner', config: this.models['ft-strategy-planner'] }
    }
    
    // Fallback to Phase 1 for other tasks
    return this.selectPhase1Model(taskType, complexity)
  }

  private static selectPhase2_5Model(taskType: string, complexity: TaskComplexity) {
    // Use Groq for speed-critical simple tasks
    if (complexity === 'simple' && taskType !== 'critique') {
      return { modelKey: 'llama-8b-groq', config: this.models['llama-8b-groq'] }
    }
    
    // Otherwise use Phase 2 logic
    return this.selectPhase2Model(taskType, complexity)
  }

  private static selectPhase3Model(taskType: string, complexity: TaskComplexity) {
    // Tier 1: Self-hosted LLaMA 8B
    if (complexity === 'simple') {
      return { modelKey: 'llama-8b-self', config: this.models['llama-8b-self'] }
    }
    
    // Tier 2: Self-hosted Mixtral or LLaMA 70B
    if (complexity === 'medium' || complexity === 'complex') {
      // Use Mixtral for most medium tasks (better cost/performance)
      if (taskType !== 'critique') {
        return { modelKey: 'mixtral-8x7b-self', config: this.models['mixtral-8x7b-self'] }
      }
    }
    
    // Tier 3: External APIs for critique and critical tasks
    if (taskType === 'critique' || complexity === 'critical') {
      return { modelKey: 'claude-3-5-sonnet', config: this.models['claude-3-5-sonnet'] }
    }
    
    // Default to Mixtral
    return { modelKey: 'mixtral-8x7b-self', config: this.models['mixtral-8x7b-self'] }
  }

  static createLLM(
    taskType: string,
    complexity: TaskComplexity,
    requiresVision: boolean = false
  ) {
    const phase = process.env.AI_PHASE || 'phase1'
    const { modelKey, config } = this.selectModel(taskType, complexity, requiresVision, phase as any)

    console.log(`[TieredReasoning] Phase: ${phase}, Task: ${taskType}, Model: ${modelKey}`)

    // Handle different providers
    switch (config.provider) {
      case 'openai':
        return new ChatOpenAI({
          modelName: config.model,
          temperature: complexity === 'simple' ? 0.1 : 0.3,
          apiKey: process.env.OPENAI_API_KEY
        })
      
      case 'anthropic':
        return new ChatAnthropic({
          modelName: config.model,
          temperature: complexity === 'simple' ? 0.1 : 0.3,
          apiKey: process.env.ANTHROPIC_API_KEY
        })
      
      case 'groq':
        return new ChatGroq({
          modelName: config.model,
          temperature: complexity === 'simple' ? 0.1 : 0.3,
          apiKey: process.env.GROQ_API_KEY
        })
      
      case 'self-hosted':
        return new ChatOpenAI({
          modelName: config.model,
          temperature: complexity === 'simple' ? 0.1 : 0.3,
          configuration: {
            baseURL: config.endpoint
          }
        })
      
      default:
        throw new Error(`Unknown provider: ${config.provider}`)
    }
  }
}
```

### 2. Environment Variables

```bash
# .env

# Phase 1: External APIs
OPENAI_API_KEY=sk-proj-xxx
ANTHROPIC_API_KEY=sk-ant-xxx

# Phase 2.5: Groq
GROQ_API_KEY=gsk_xxx

# Phase 3: Self-hosted endpoints
LLAMA_8B_ENDPOINT=http://llama-8b.internal:8000/v1
MIXTRAL_ENDPOINT=http://mixtral.internal:8000/v1
LLAMA_70B_ENDPOINT=http://llama-70b.internal:8000/v1

# Current phase
AI_PHASE=phase1  # Options: phase1, phase2, phase2.5, phase3
```

### 3. Self-Hosted Model Serving (Phase 3)

```yaml
# docker-compose.yml for self-hosted models

version: '3.8'

services:
  llama-8b:
    image: vllm/vllm-openai:latest
    command: >
      --model meta-llama/Llama-3.1-8B-Instruct
      --tensor-parallel-size 1
      --max-model-len 8192
      --port 8000
    ports:
      - "8001:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}

  mixtral-8x7b:
    image: vllm/vllm-openai:latest
    command: >
      --model mistralai/Mixtral-8x7B-Instruct-v0.1
      --tensor-parallel-size 2
      --max-model-len 16384
      --port 8000
    ports:
      - "8002:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}

  llama-70b:
    image: vllm/vllm-openai:latest
    command: >
      --model meta-llama/Llama-3.1-70B-Instruct
      --tensor-parallel-size 4
      --max-model-len 8192
      --port 8000
    ports:
      - "8003:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 4
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
```

---

## Decision Framework

### When to Use Each Phase:

**Phase 1 (External APIs Only):**
- âœ… Just starting out (0-3 months)
- âœ… < 10k tests/month
- âœ… No ML/DevOps team
- âœ… Need to move fast
- âœ… API costs < $15k/month

**Phase 2 (Fine-Tuned APIs):**
- âœ… Have 1,000+ labeled tests
- âœ… 10k-50k tests/month
- âœ… API costs $15k-50k/month
- âœ… Want 30-50% cost reduction
- âœ… Still want simplicity (no infrastructure)

**Phase 2.5 (+ Groq):**
- âœ… Need faster inference
- âœ… Real-time features important
- âœ… Want cost savings without infrastructure
- âœ… 20k-100k tests/month

**Phase 3 (Hybrid Self-Hosted):**
- âœ… 50k+ tests/month
- âœ… API costs > $50k/month
- âœ… Have ML/DevOps team
- âœ… Want maximum control
- âœ… Data privacy is critical
- âœ… Can manage infrastructure

---

## Roadmap Timeline

```
Month 0-3: Phase 1 âœ…
â”œâ”€ Use external APIs
â”œâ”€ Implement tiered reasoning
â”œâ”€ Collect training data
â””â”€ Goal: 1,000 labeled tests

Month 3-6: Phase 2 Start ğŸ¯
â”œâ”€ Build training pipeline
â”œâ”€ Fine-tune first models
â”œâ”€ A/B test performance
â””â”€ Goal: 30% cost reduction

Month 6-9: Phase 2 Scale
â”œâ”€ Fine-tune more models
â”œâ”€ Automate retraining
â”œâ”€ Add Groq (Phase 2.5)
â””â”€ Goal: 50% cost reduction

Month 9-12: Phase 3 Planning
â”œâ”€ Evaluate self-hosting ROI
â”œâ”€ Build infrastructure
â”œâ”€ Deploy LLaMA 8B
â””â”€ Goal: Proof of concept

Month 12-18: Phase 3 Scale
â”œâ”€ Deploy Mixtral 8x7B
â”œâ”€ Migrate 70% to self-hosted
â”œâ”€ Keep 30% on external APIs
â””â”€ Goal: 60% cost reduction
```

---

## Key Takeaways

1. **Start Simple:** Phase 1 with external APIs (already done âœ…)

2. **Fine-Tune Next:** Phase 2 gives 30-50% savings with no infrastructure

3. **Add Groq for Speed:** Phase 2.5 is easy win for performance

4. **Self-Host at Scale:** Phase 3 only makes sense at 50k+ tests/month

5. **Always Keep Tier 3 External:** Claude/GPT-4o for vision and critique

6. **The Flywheel Works:** More tests â†’ Better models â†’ Lower costs â†’ More customers

7. **You're Not Training OpenAI:** You're fine-tuning YOUR versions of their models

8. **Open Source is Real:** Mixtral and LLaMA are production-ready alternatives

---

## Next Actions

### Immediate (This Week):
1. Set `AI_PHASE=phase1` in environment
2. Ensure training data collection is working
3. Verify tiered reasoning is active

### Next Month:
1. Build training data export pipeline
2. Fine-tune first model (issue detector)
3. A/B test against baseline

### Next Quarter:
1. Scale fine-tuning to 4+ models
2. Evaluate Groq integration
3. Plan Phase 3 if volume justifies it

---

**The hybrid solution is ready. Start with Phase 1 (done), move to Phase 2 (fine-tuning), optionally add Phase 2.5 (Groq), and only go to Phase 3 (self-hosted) when volume justifies the complexity.**
