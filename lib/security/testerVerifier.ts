import { ChatAnthropic } from '@langchain/anthropic'

interface VerificationResult {
  isAuthentic: boolean
  confidence: number
  suspiciousPatterns: string[]
  reasoning: string
  recommendation: 'accept' | 'review' | 'reject'
}

/**
 * TesterVerifier - Ensures human testers aren't using AI to write reports
 * 
 * Protects the integrity of "hybrid" testing data by detecting if a tester
 * used ChatGPT or other AI to generate their friction point reports instead
 * of actually testing the site.
 */
export class TesterVerifier {
  private llm: ChatAnthropic

  constructor() {
    this.llm = new ChatAnthropic({
      modelName: 'claude-3-5-sonnet-20241022',
      temperature: 0.1,
      apiKey: process.env.ANTHROPIC_API_KEY
    })
  }

  /**
   * Analyzes tester submission for AI-generated content
   */
  async verifySubmission(
    testerReport: {
      frictionPoints: Array<{
        element: string
        issue: string
        severity: string
      }>
      notes: string
      completionTime: number
    },
    testContext: {
      url: string
      objective: string
      persona: any
    }
  ): Promise<VerificationResult> {
    const prompt = `You are a fraud detection specialist. Analyze this user testing report to determine if it was written by a real human tester or generated by AI (ChatGPT, Claude, etc.).

**Test Context:**
- URL: ${testContext.url}
- Objective: ${testContext.objective}
- Persona: ${testContext.persona.name}
- Completion Time: ${testerReport.completionTime}s

**Tester's Report:**
${JSON.stringify(testerReport, null, 2)}

**Red Flags for AI-Generated Content:**
1. **Overly formal/perfect language** - Real users write casually, with typos
2. **Generic observations** - AI gives textbook answers, humans give specific details
3. **Lack of personal frustration** - Real users say "I couldn't find..." not "The user might struggle..."
4. **Perfect structure** - AI uses consistent formatting, humans are messy
5. **No specific timestamps/sequences** - AI doesn't mention "after clicking X, then Y happened"
6. **Buzzword overuse** - Terms like "cognitive load", "heuristics", "affordance" (unless tester is UX pro)
7. **Too comprehensive** - Real users miss things, AI finds everything
8. **Completion time mismatch** - If report is very detailed but completion time is 30 seconds

**Green Flags for Authentic Content:**
1. **Casual language** - "This button is super confusing", "I had no idea where to click"
2. **Specific details** - "The blue button in the top right", "After I scrolled down..."
3. **Personal frustration** - "I got annoyed", "This made me want to give up"
4. **Typos/informal writing** - "didnt work", "kinda hard to see"
5. **Incomplete observations** - Missing some issues (humans aren't perfect)
6. **Realistic completion time** - Matches the depth of report

**Your Task:**
Determine if this is authentic human testing or AI-generated.

Return JSON:
{
  "isAuthentic": true/false,
  "confidence": 0.0-1.0,
  "suspiciousPatterns": ["pattern1", "pattern2"],
  "reasoning": "detailed explanation",
  "recommendation": "accept|review|reject"
}`

    try {
      const response = await this.llm.invoke(prompt)
      const content = response.content as string
      
      const jsonMatch = content.match(/\{[\s\S]*\}/)
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0])
      }

      throw new Error('Failed to parse verification result')
    } catch (error) {
      console.error('Verification failed:', error)
      return {
        isAuthentic: true, // Fail open to avoid blocking legitimate testers
        confidence: 0.5,
        suspiciousPatterns: [],
        reasoning: 'Verification system error',
        recommendation: 'review'
      }
    }
  }

  /**
   * Analyzes patterns across multiple submissions from same tester
   * Detects if they're consistently using AI
   */
  async analyzeTestPatterns(
    testerSubmissions: Array<{
      testId: string
      report: any
      timestamp: Date
    }>
  ): Promise<{
    suspiciousActivity: boolean
    patterns: string[]
    trustScore: number
  }> {
    if (testerSubmissions.length < 3) {
      return {
        suspiciousActivity: false,
        patterns: [],
        trustScore: 1.0
      }
    }

    const verifications = await Promise.all(
      testerSubmissions.map(sub => 
        this.verifySubmission(sub.report, {
          url: 'N/A',
          objective: 'N/A',
          persona: { name: 'N/A' }
        })
      )
    )

    const avgConfidence = verifications.reduce((sum, v) => sum + v.confidence, 0) / verifications.length
    const authenticCount = verifications.filter(v => v.isAuthentic).length
    const trustScore = authenticCount / verifications.length

    const allPatterns = verifications.flatMap(v => v.suspiciousPatterns)
    const patternCounts = new Map<string, number>()
    allPatterns.forEach(p => patternCounts.set(p, (patternCounts.get(p) || 0) + 1))

    const repeatedPatterns = Array.from(patternCounts.entries())
      .filter(([_, count]) => count >= 2)
      .map(([pattern, _]) => pattern)

    return {
      suspiciousActivity: trustScore < 0.5 || repeatedPatterns.length > 3,
      patterns: repeatedPatterns,
      trustScore
    }
  }

  /**
   * Generates a verification report for admin review
   */
  async generateVerificationReport(
    testerId: string,
    recentSubmissions: any[]
  ): Promise<string> {
    const analysis = await this.analyzeTestPatterns(recentSubmissions)

    return `
# Tester Verification Report

**Tester ID:** ${testerId}
**Submissions Analyzed:** ${recentSubmissions.length}
**Trust Score:** ${(analysis.trustScore * 100).toFixed(1)}%

## Assessment
${analysis.suspiciousActivity 
  ? '‚ö†Ô∏è **SUSPICIOUS ACTIVITY DETECTED**' 
  : '‚úÖ **No suspicious activity detected**'
}

## Patterns Detected
${analysis.patterns.length > 0 
  ? analysis.patterns.map(p => `- ${p}`).join('\n')
  : 'No concerning patterns'
}

## Recommendation
${analysis.trustScore > 0.8 
  ? '‚úÖ Tester appears authentic - continue normal operations'
  : analysis.trustScore > 0.5
    ? '‚ö†Ô∏è Review tester submissions manually - some concerns'
    : 'üö´ High risk of AI-generated reports - consider suspension'
}
`
  }
}
